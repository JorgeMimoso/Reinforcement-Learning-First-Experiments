{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "yellow-commerce",
   "metadata": {
    "id": "yellow-commerce"
   },
   "source": [
    "# 0. Install Dependencies\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "western-citation",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35977,
     "status": "ok",
     "timestamp": 1679582413782,
     "user": {
      "displayName": "Jorge Mimoso",
      "userId": "07607860909645255176"
     },
     "user_tz": 0
    },
    "id": "western-citation",
    "outputId": "3b552787-8e33-4a5a-b927-5eee61e31ca4"
   },
   "outputs": [],
   "source": [
    "#!pip install rl-agents==0.1.1\n",
    "#!pip install tensorflow\n",
    "#!pip install gym\n",
    "#!pip install keras\n",
    "#!pip install keras-rl2\n",
    "#!pip install pygame\n",
    "#conda install swig -> this should be installed in the Anaconda command line\n",
    "#conda install -c conda-forge box2d-py ->  this should be installed in the Anaconda command line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accomplished-armenia",
   "metadata": {
    "id": "accomplished-armenia"
   },
   "source": [
    "# 1. Test Random Environment with OpenAI Gym\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hindu-ceremony",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4502,
     "status": "ok",
     "timestamp": 1679582555308,
     "user": {
      "displayName": "Jorge Mimoso",
      "userId": "07607860909645255176"
     },
     "user_tz": 0
    },
    "id": "hindu-ceremony",
    "outputId": "5c29a6db-05ed-4d6f-f742-1f3288c6e030"
   },
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "#print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "import gym\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "russian-nudist",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 651,
     "status": "ok",
     "timestamp": 1679592616304,
     "user": {
      "displayName": "Jorge Mimoso",
      "userId": "07607860909645255176"
     },
     "user_tz": 0
    },
    "id": "russian-nudist",
    "outputId": "6e5c79e7-aa7a-4deb-fa78-c3903d2dc08e"
   },
   "outputs": [],
   "source": [
    "#Game being used\n",
    "env = gym.make('LunarLander-v2')# ->https://gymnasium.farama.org/environments/box2d/lunar_lander/\n",
    "\n",
    "\n",
    "states = env.observation_space.shape[0]\n",
    "\n",
    "#Number of actions available.\n",
    "actions = env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "detected-registration",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 259,
     "status": "ok",
     "timestamp": 1679582569422,
     "user": {
      "displayName": "Jorge Mimoso",
      "userId": "07607860909645255176"
     },
     "user_tz": 0
    },
    "id": "detected-registration",
    "outputId": "bf55b49e-4968-4e16-ca03-a1192c99db27",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(states)\n",
    "print(actions.n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "atmospheric-intersection",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 953
    },
    "executionInfo": {
     "elapsed": 15705,
     "status": "ok",
     "timestamp": 1679592641808,
     "user": {
      "displayName": "Jorge Mimoso",
      "userId": "07607860909645255176"
     },
     "user_tz": 0
    },
    "id": "atmospheric-intersection",
    "outputId": "ece43b76-34d6-4523-f391-c062856b42e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:-87.22762958512209\n",
      "Episode:2 Score:-410.6787038564532\n",
      "Episode:3 Score:-138.90818251647892\n",
      "Episode:4 Score:-172.8068527250178\n",
      "Episode:5 Score:-173.8826573272541\n",
      "Episode:6 Score:-299.6772528406517\n",
      "Episode:7 Score:-120.2283145199923\n",
      "Episode:8 Score:-99.33083343746699\n",
      "Episode:9 Score:-122.7421905798728\n",
      "Episode:10 Score:-113.5958499647744\n"
     ]
    }
   ],
   "source": [
    "#Env setting with random behaviour\n",
    "episodes = 10\n",
    "for episode in range(1,episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "        \n",
    "    while not done:\n",
    "        env.render()\n",
    "        n_state, reward, done, info = env.step(env.action_space.sample())\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'.format(episode,score))\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-edgar",
   "metadata": {
    "id": "blocked-edgar"
   },
   "source": [
    "# 2. Create a Deep Learning Model with Keras\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "infectious-smile",
   "metadata": {
    "executionInfo": {
     "elapsed": 2595,
     "status": "ok",
     "timestamp": 1679582618702,
     "user": {
      "displayName": "Jorge Mimoso",
      "userId": "07607860909645255176"
     },
     "user_tz": 0
    },
    "id": "infectious-smile"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Jorge Mimoso\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Jorge Mimoso\\anaconda3\\Lib\\site-packages\\rl\\agents\\ddpg.py:9: The name tf.disable_eager_execution is deprecated. Please use tf.compat.v1.disable_eager_execution instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Flatten\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "\n",
    "from keras import __version__\n",
    "tf.keras.__version__ = __version__\n",
    "\n",
    "#https://www.tensorflow.org/agents/api_docs/python/tf_agents/agents/DdpgAgent?hl=en\n",
    "#from rl.agents import SARSAAgent #check diferent agents -> https://keras-rl.readthedocs.io/en/latest/\n",
    "from rl.agents import DQNAgent #check diferent agents -> https://keras-rl.readthedocs.io/en/latest/\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4pGN9CZ8D9-c",
   "metadata": {
    "executionInfo": {
     "elapsed": 244,
     "status": "ok",
     "timestamp": 1679592647800,
     "user": {
      "displayName": "Jorge Mimoso",
      "userId": "07607860909645255176"
     },
     "user_tz": 0
    },
    "id": "4pGN9CZ8D9-c"
   },
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    model = Sequential() \n",
    "    model.add(Flatten(input_shape=(1,) + env.observation_space.shape)) \n",
    "    model.add(Dense(200,activation='relu'))\n",
    "    model.add(Dense(200,activation='relu'))\n",
    "    model.add(Dense(actions,activation='linear'))    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "jboF2gqFXE20",
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1679587076588,
     "user": {
      "displayName": "Jorge Mimoso",
      "userId": "07607860909645255176"
     },
     "user_tz": 0
    },
    "id": "jboF2gqFXE20"
   },
   "outputs": [],
   "source": [
    "#del model #-> Uncoment if Sequential error appears after building the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "E6yWRKzUEHl2",
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1679592649813,
     "user": {
      "displayName": "Jorge Mimoso",
      "userId": "07607860909645255176"
     },
     "user_tz": 0
    },
    "id": "E6yWRKzUEHl2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Jorge Mimoso\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\version_utils.py:76: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = build_model(states,actions.n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1PGADNmqEJYj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 293,
     "status": "ok",
     "timestamp": 1679588888046,
     "user": {
      "displayName": "Jorge Mimoso",
      "userId": "07607860909645255176"
     },
     "user_tz": 0
    },
    "id": "1PGADNmqEJYj",
    "outputId": "dd06bce0-e780-4dd1-c705-393f759b533f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 8)                 0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 200)               1800      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 200)               40200     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 4)                 804       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 42804 (167.20 KB)\n",
      "Trainable params: 42804 (167.20 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sxjw_l-RIQDu",
   "metadata": {
    "id": "sxjw_l-RIQDu"
   },
   "source": [
    "# 3. Build Agent with Keras Neural Network\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "Ny-QPwcrEOHk",
   "metadata": {
    "executionInfo": {
     "elapsed": 203,
     "status": "ok",
     "timestamp": 1679592658294,
     "user": {
      "displayName": "Jorge Mimoso",
      "userId": "07607860909645255176"
     },
     "user_tz": 0
    },
    "id": "Ny-QPwcrEOHk"
   },
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = EpsGreedyQPolicy()\n",
    "    #earlystop = EarlyStopping(monitor = 'episode_reward', min_delta=.1, patience=5, verbose=1, mode='auto') \n",
    "    memory = SequentialMemory(limit=50000,window_length=1)\n",
    "    #callbacks = [earlystop] \n",
    "    nb_steps_warmup = 1000 \n",
    "    target_model_update = .02 \n",
    "    #gamma = .99 \n",
    "    #epochs = training_steps/1000 \n",
    "    #decay = float(lr/epochs) \n",
    "    dqn = DQNAgent(model=model, nb_actions=actions, memory=memory, nb_steps_warmup=nb_steps_warmup, target_model_update = target_model_update, policy=policy)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JWH3hy49CWm9",
   "metadata": {
    "executionInfo": {
     "elapsed": 2038,
     "status": "ok",
     "timestamp": 1679592664324,
     "user": {
      "displayName": "Jorge Mimoso",
      "userId": "07607860909645255176"
     },
     "user_tz": 0
    },
    "id": "JWH3hy49CWm9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Jorge Mimoso\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Training for 2000000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "WARNING:tensorflow:From C:\\Users\\Jorge Mimoso\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training_v1.py:2595: The name tf.data.Iterator is deprecated. Please use tf.compat.v1.data.Iterator instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Jorge Mimoso\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training_utils_v1.py:50: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "  243/10000 [..............................] - ETA: 6s - reward: -4.4510"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jorge Mimoso\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 64s 6ms/step - reward: -1.5034\n",
      "66 episodes - episode_reward: -227.677 [-1317.158, 270.371] - loss: 20.257 - mae: 14.597 - mean_q: -16.980\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 161s 16ms/step - reward: -0.4615\n",
      "41 episodes - episode_reward: -112.587 [-207.230, 199.116] - loss: 15.823 - mae: 16.921 - mean_q: -5.913\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 175s 18ms/step - reward: -0.4022\n",
      "27 episodes - episode_reward: -148.252 [-298.076, 210.489] - loss: 9.883 - mae: 24.083 - mean_q: -0.509\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 188s 19ms/step - reward: -0.1236\n",
      "11 episodes - episode_reward: -116.290 [-284.742, -48.945] - loss: 7.375 - mae: 27.945 - mean_q: 16.325\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 182s 18ms/step - reward: -0.0249\n",
      "10 episodes - episode_reward: -29.039 [-97.411, 161.585] - loss: 6.071 - mae: 29.228 - mean_q: 25.087\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 191s 19ms/step - reward: -0.0547\n",
      "10 episodes - episode_reward: -49.468 [-95.597, 1.592] - loss: 3.402 - mae: 25.861 - mean_q: 25.156\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 183s 18ms/step - reward: -0.0891\n",
      "11 episodes - episode_reward: -74.742 [-131.813, 16.006] - loss: 2.497 - mae: 24.860 - mean_q: 29.744\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 187s 19ms/step - reward: -0.1115\n",
      "15 episodes - episode_reward: -77.550 [-210.113, 174.837] - loss: 2.195 - mae: 24.145 - mean_q: 31.072\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 193s 19ms/step - reward: -0.0785\n",
      "10 episodes - episode_reward: -73.453 [-174.601, -31.174] - loss: 1.786 - mae: 19.266 - mean_q: 25.110\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 196s 20ms/step - reward: -0.0200\n",
      "10 episodes - episode_reward: -25.532 [-84.011, 42.553] - loss: 1.475 - mae: 15.458 - mean_q: 20.045\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 203s 20ms/step - reward: -0.0179\n",
      "10 episodes - episode_reward: -19.604 [-78.264, 17.117] - loss: 1.574 - mae: 14.809 - mean_q: 19.162\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 194s 19ms/step - reward: -0.0513\n",
      "12 episodes - episode_reward: -39.952 [-200.800, 95.413] - loss: 1.794 - mae: 15.308 - mean_q: 19.803\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 194s 19ms/step - reward: -0.0186\n",
      "12 episodes - episode_reward: -18.295 [-251.203, 155.560] - loss: 1.688 - mae: 14.331 - mean_q: 18.833\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 193s 19ms/step - reward: -0.0230\n",
      "12 episodes - episode_reward: -20.620 [-130.424, 139.130] - loss: 1.745 - mae: 14.956 - mean_q: 19.653\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 198s 20ms/step - reward: 0.0270\n",
      "12 episodes - episode_reward: 21.868 [-109.135, 273.902] - loss: 1.689 - mae: 11.659 - mean_q: 15.374\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 194s 19ms/step - reward: 0.0492\n",
      "13 episodes - episode_reward: 40.340 [-59.342, 245.244] - loss: 2.372 - mae: 12.883 - mean_q: 17.061\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 188s 19ms/step - reward: 0.0821\n",
      "13 episodes - episode_reward: 59.933 [-109.321, 231.622] - loss: 3.314 - mae: 14.089 - mean_q: 18.738\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 181s 18ms/step - reward: 0.1985\n",
      "14 episodes - episode_reward: 141.017 [-73.398, 261.604] - loss: 3.860 - mae: 17.768 - mean_q: 23.832\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 123s 12ms/step - reward: 0.1885\n",
      "17 episodes - episode_reward: 108.368 [-295.161, 259.082] - loss: 4.252 - mae: 21.938 - mean_q: 29.421\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 140s 14ms/step - reward: 0.3335\n",
      "20 episodes - episode_reward: 169.339 [-117.612, 294.912] - loss: 5.539 - mae: 25.050 - mean_q: 33.439\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 92s 9ms/step - reward: 0.2032\n",
      "15 episodes - episode_reward: 138.314 [-92.097, 286.156] - loss: 5.559 - mae: 25.414 - mean_q: 34.160\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      "10000/10000 [==============================] - 86s 9ms/step - reward: 0.2519\n",
      "17 episodes - episode_reward: 146.451 [-59.658, 239.262] - loss: 5.049 - mae: 27.280 - mean_q: 36.760\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      "10000/10000 [==============================] - 139s 14ms/step - reward: 0.2594\n",
      "15 episodes - episode_reward: 168.200 [-60.892, 251.839] - loss: 5.063 - mae: 28.876 - mean_q: 38.920\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      "10000/10000 [==============================] - 89s 9ms/step - reward: 0.3062\n",
      "20 episodes - episode_reward: 155.171 [-88.016, 296.618] - loss: 5.208 - mae: 29.202 - mean_q: 39.381\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      "10000/10000 [==============================] - 89s 9ms/step - reward: 0.3124\n",
      "16 episodes - episode_reward: 190.802 [87.048, 264.550] - loss: 4.701 - mae: 28.087 - mean_q: 37.940\n",
      "\n",
      "Interval 26 (250000 steps performed)\n",
      "10000/10000 [==============================] - 84s 8ms/step - reward: 0.6228\n",
      "25 episodes - episode_reward: 249.030 [199.996, 304.419] - loss: 4.369 - mae: 30.632 - mean_q: 41.321\n",
      "\n",
      "Interval 27 (260000 steps performed)\n",
      "10000/10000 [==============================] - 85s 9ms/step - reward: 0.5748\n",
      "25 episodes - episode_reward: 229.945 [7.365, 287.895] - loss: 4.358 - mae: 37.466 - mean_q: 50.468\n",
      "\n",
      "Interval 28 (270000 steps performed)\n",
      "10000/10000 [==============================] - 81s 8ms/step - reward: 0.4324\n",
      "22 episodes - episode_reward: 195.563 [-48.117, 274.832] - loss: 4.759 - mae: 38.583 - mean_q: 52.000\n",
      "\n",
      "Interval 29 (280000 steps performed)\n",
      "10000/10000 [==============================] - 85s 9ms/step - reward: 0.6964\n",
      "28 episodes - episode_reward: 254.130 [147.457, 294.802] - loss: 4.563 - mae: 39.398 - mean_q: 53.070\n",
      "\n",
      "Interval 30 (290000 steps performed)\n",
      "10000/10000 [==============================] - 85s 9ms/step - reward: 0.6906\n",
      "26 episodes - episode_reward: 266.073 [239.545, 291.072] - loss: 3.934 - mae: 43.317 - mean_q: 58.278\n",
      "\n",
      "Interval 31 (300000 steps performed)\n",
      "10000/10000 [==============================] - 83s 8ms/step - reward: 0.7223\n",
      "29 episodes - episode_reward: 244.233 [-3.371, 291.213] - loss: 3.947 - mae: 45.984 - mean_q: 61.826\n",
      "\n",
      "Interval 32 (310000 steps performed)\n",
      "10000/10000 [==============================] - 86s 9ms/step - reward: 0.7086\n",
      "27 episodes - episode_reward: 265.198 [216.943, 302.499] - loss: 3.804 - mae: 47.707 - mean_q: 64.139\n",
      "\n",
      "Interval 33 (320000 steps performed)\n",
      "10000/10000 [==============================] - 84s 8ms/step - reward: 0.7074\n",
      "28 episodes - episode_reward: 255.347 [78.092, 309.458] - loss: 3.388 - mae: 49.401 - mean_q: 66.386\n",
      "\n",
      "Interval 34 (330000 steps performed)\n",
      "10000/10000 [==============================] - 85s 9ms/step - reward: 0.8032\n",
      "32 episodes - episode_reward: 245.661 [-167.614, 300.724] - loss: 3.505 - mae: 52.398 - mean_q: 70.453\n",
      "\n",
      "Interval 35 (340000 steps performed)\n",
      "10000/10000 [==============================] - 85s 9ms/step - reward: 0.8520\n",
      "33 episodes - episode_reward: 261.979 [216.816, 300.633] - loss: 3.641 - mae: 54.548 - mean_q: 73.276\n",
      "\n",
      "Interval 36 (350000 steps performed)\n",
      "10000/10000 [==============================] - 85s 9ms/step - reward: 0.8452\n",
      "32 episodes - episode_reward: 259.337 [61.054, 308.259] - loss: 4.611 - mae: 55.709 - mean_q: 74.793\n",
      "\n",
      "Interval 37 (360000 steps performed)\n",
      "10000/10000 [==============================] - 86s 9ms/step - reward: 0.8447\n",
      "33 episodes - episode_reward: 259.265 [18.049, 310.307] - loss: 4.355 - mae: 56.008 - mean_q: 75.200\n",
      "\n",
      "Interval 38 (370000 steps performed)\n",
      "10000/10000 [==============================] - 85s 9ms/step - reward: 0.6974\n",
      "28 episodes - episode_reward: 246.806 [-0.607, 296.614] - loss: 4.909 - mae: 56.639 - mean_q: 76.009\n",
      "\n",
      "Interval 39 (380000 steps performed)\n",
      "10000/10000 [==============================] - 88s 9ms/step - reward: 0.5952\n",
      "23 episodes - episode_reward: 257.106 [185.283, 296.745] - loss: 4.448 - mae: 55.110 - mean_q: 74.019\n",
      "\n",
      "Interval 40 (390000 steps performed)\n",
      "10000/10000 [==============================] - 85s 8ms/step - reward: 0.7428\n",
      "29 episodes - episode_reward: 262.394 [197.362, 303.348] - loss: 4.733 - mae: 53.033 - mean_q: 71.422\n",
      "\n",
      "Interval 41 (400000 steps performed)\n",
      "10000/10000 [==============================] - 60s 6ms/step - reward: 0.8476\n",
      "31 episodes - episode_reward: 268.377 [208.190, 307.684] - loss: 4.558 - mae: 52.640 - mean_q: 70.924\n",
      "\n",
      "Interval 42 (410000 steps performed)\n",
      "10000/10000 [==============================] - 61s 6ms/step - reward: 0.7517\n",
      "30 episodes - episode_reward: 252.256 [41.463, 306.761] - loss: 4.199 - mae: 53.271 - mean_q: 71.752\n",
      "\n",
      "Interval 43 (420000 steps performed)\n",
      "10000/10000 [==============================] - 61s 6ms/step - reward: 0.7382\n",
      "32 episodes - episode_reward: 228.630 [19.523, 307.903] - loss: 4.923 - mae: 53.283 - mean_q: 71.823\n",
      "\n",
      "Interval 44 (430000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 0.8107\n",
      "31 episodes - episode_reward: 261.791 [26.307, 312.615] - loss: 5.400 - mae: 53.439 - mean_q: 72.024\n",
      "\n",
      "Interval 45 (440000 steps performed)\n",
      "10000/10000 [==============================] - 60s 6ms/step - reward: 0.7806\n",
      "29 episodes - episode_reward: 271.079 [227.875, 297.926] - loss: 4.978 - mae: 54.065 - mean_q: 72.749\n",
      "\n",
      "Interval 46 (450000 steps performed)\n",
      "10000/10000 [==============================] - 61s 6ms/step - reward: 0.8570\n",
      "32 episodes - episode_reward: 267.481 [226.644, 300.263] - loss: 5.035 - mae: 54.667 - mean_q: 73.540\n",
      "\n",
      "Interval 47 (460000 steps performed)\n",
      "10000/10000 [==============================] - 58s 6ms/step - reward: 0.9239\n",
      "35 episodes - episode_reward: 266.792 [103.260, 306.929] - loss: 4.938 - mae: 56.063 - mean_q: 75.467\n",
      "\n",
      "Interval 48 (470000 steps performed)\n",
      "10000/10000 [==============================] - 59s 6ms/step - reward: 0.7729\n",
      "29 episodes - episode_reward: 261.545 [111.114, 306.573] - loss: 4.042 - mae: 56.136 - mean_q: 75.500\n",
      "\n",
      "Interval 49 (480000 steps performed)\n",
      "10000/10000 [==============================] - 59s 6ms/step - reward: 0.8213\n",
      "32 episodes - episode_reward: 261.194 [26.841, 301.758] - loss: 4.143 - mae: 56.475 - mean_q: 75.972\n",
      "\n",
      "Interval 50 (490000 steps performed)\n",
      "10000/10000 [==============================] - 58s 6ms/step - reward: 0.9172\n",
      "36 episodes - episode_reward: 250.830 [31.704, 304.855] - loss: 4.202 - mae: 57.003 - mean_q: 76.705\n",
      "\n",
      "Interval 51 (500000 steps performed)\n",
      "10000/10000 [==============================] - 58s 6ms/step - reward: 0.7426\n",
      "30 episodes - episode_reward: 247.734 [17.301, 320.577] - loss: 5.348 - mae: 57.054 - mean_q: 76.808\n",
      "\n",
      "Interval 52 (510000 steps performed)\n",
      "10000/10000 [==============================] - 59s 6ms/step - reward: 0.7474\n",
      "29 episodes - episode_reward: 257.152 [75.657, 303.192] - loss: 5.412 - mae: 55.190 - mean_q: 74.368\n",
      "\n",
      "Interval 53 (520000 steps performed)\n",
      "10000/10000 [==============================] - 59s 6ms/step - reward: 0.8092\n",
      "30 episodes - episode_reward: 274.446 [229.543, 299.976] - loss: 5.457 - mae: 54.805 - mean_q: 73.866\n",
      "\n",
      "Interval 54 (530000 steps performed)\n",
      "10000/10000 [==============================] - 61s 6ms/step - reward: 0.8569\n",
      "32 episodes - episode_reward: 262.903 [20.621, 304.295] - loss: 4.924 - mae: 55.446 - mean_q: 74.723\n",
      "\n",
      "Interval 55 (540000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 1.0127\n",
      "38 episodes - episode_reward: 270.649 [72.234, 303.353] - loss: 4.938 - mae: 56.070 - mean_q: 75.489\n",
      "\n",
      "Interval 56 (550000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 0.8473\n",
      "32 episodes - episode_reward: 260.080 [22.217, 311.803] - loss: 3.989 - mae: 58.224 - mean_q: 78.403\n",
      "\n",
      "Interval 57 (560000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: 0.9105\n",
      "35 episodes - episode_reward: 259.829 [21.880, 317.530] - loss: 3.947 - mae: 59.389 - mean_q: 79.942\n",
      "\n",
      "Interval 58 (570000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 0.8936\n",
      "33 episodes - episode_reward: 275.179 [180.874, 315.854] - loss: 4.136 - mae: 60.549 - mean_q: 81.494\n",
      "\n",
      "Interval 59 (580000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 0.8675\n",
      "33 episodes - episode_reward: 262.656 [-7.370, 312.978] - loss: 4.497 - mae: 59.612 - mean_q: 80.216\n",
      "\n",
      "Interval 60 (590000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: 0.7362\n",
      "29 episodes - episode_reward: 254.993 [-138.961, 303.523] - loss: 4.423 - mae: 58.917 - mean_q: 79.341\n",
      "\n",
      "Interval 61 (600000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 0.9575\n",
      "37 episodes - episode_reward: 254.083 [-38.180, 317.556] - loss: 4.648 - mae: 59.664 - mean_q: 80.296\n",
      "\n",
      "Interval 62 (610000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 0.7380\n",
      "30 episodes - episode_reward: 247.440 [2.433, 300.722] - loss: 5.418 - mae: 58.928 - mean_q: 79.241\n",
      "\n",
      "Interval 63 (620000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 0.7678\n",
      "30 episodes - episode_reward: 258.631 [101.010, 307.449] - loss: 5.199 - mae: 58.083 - mean_q: 78.183\n",
      "\n",
      "Interval 64 (630000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 0.8636\n",
      "31 episodes - episode_reward: 276.245 [183.693, 314.172] - loss: 4.988 - mae: 57.164 - mean_q: 77.000\n",
      "\n",
      "Interval 65 (640000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 0.8272\n",
      "32 episodes - episode_reward: 257.897 [-206.706, 309.327] - loss: 4.806 - mae: 60.462 - mean_q: 81.465\n",
      "\n",
      "Interval 66 (650000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 0.8389\n",
      "34 episodes - episode_reward: 248.921 [37.417, 305.505] - loss: 4.478 - mae: 59.662 - mean_q: 80.388\n",
      "\n",
      "Interval 67 (660000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 1.0569\n",
      "38 episodes - episode_reward: 277.152 [238.640, 319.773] - loss: 4.647 - mae: 58.904 - mean_q: 79.344\n",
      "\n",
      "Interval 68 (670000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 0.7047\n",
      "27 episodes - episode_reward: 258.144 [31.672, 305.370] - loss: 4.915 - mae: 59.352 - mean_q: 79.848\n",
      "\n",
      "Interval 69 (680000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 0.9677\n",
      "37 episodes - episode_reward: 266.017 [31.000, 317.129] - loss: 4.847 - mae: 58.425 - mean_q: 78.539\n",
      "\n",
      "Interval 70 (690000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 0.9518\n",
      "34 episodes - episode_reward: 274.644 [237.546, 320.922] - loss: 5.202 - mae: 58.528 - mean_q: 78.857\n",
      "\n",
      "Interval 71 (700000 steps performed)\n",
      " 6688/10000 [===================>..........] - ETA: 18s - reward: 0.9120"
     ]
    }
   ],
   "source": [
    " #Adam._name = 'hey' ## use in case of error mentioning this parameter as null-\n",
    "dqn = build_agent(model,actions.n)\n",
    "\n",
    "lr = .0001 \n",
    "dqn.compile(Adam(learning_rate=lr), metrics=['mae'],)\n",
    "\n",
    "training_steps = 1000000\n",
    "\n",
    "dqn.fit(env, nb_steps=training_steps, visualize=False, verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Fd_JxTR2EOwi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52082,
     "status": "ok",
     "timestamp": 1679592722687,
     "user": {
      "displayName": "Jorge Mimoso",
      "userId": "07607860909645255176"
     },
     "user_tz": 0
    },
    "id": "Fd_JxTR2EOwi",
    "outputId": "b07d1b70-db9e-4657-c680-6a0524e50503"
   },
   "outputs": [],
   "source": [
    "scores = dqn.test(env, nb_episodes=100, visualize=False)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "G7kb8LE5I8tB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "executionInfo": {
     "elapsed": 27345,
     "status": "error",
     "timestamp": 1679587011164,
     "user": {
      "displayName": "Jorge Mimoso",
      "userId": "07607860909645255176"
     },
     "user_tz": 0
    },
    "id": "G7kb8LE5I8tB",
    "outputId": "f93c47bc-254e-4565-f66c-f3f46c53968e"
   },
   "outputs": [],
   "source": [
    "#lets test again with more 15 episodes\n",
    "_ = dqn.test(env, nb_episodes=15, visualize=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lTah1BG31RTk",
   "metadata": {
    "executionInfo": {
     "elapsed": 665,
     "status": "ok",
     "timestamp": 1679592419292,
     "user": {
      "displayName": "Jorge Mimoso",
      "userId": "07607860909645255176"
     },
     "user_tz": 0
    },
    "id": "lTah1BG31RTk"
   },
   "outputs": [],
   "source": [
    "dqn.save_weights('LunarLander-v2_weights.h5f',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lqFOWVsZ-oIW",
   "metadata": {
    "id": "lqFOWVsZ-oIW"
   },
   "outputs": [],
   "source": [
    "#Amazing"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

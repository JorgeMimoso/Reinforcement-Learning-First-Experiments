{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "yellow-commerce",
   "metadata": {
    "id": "yellow-commerce"
   },
   "source": [
    "# 0. Install Dependencies\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "western-citation",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35977,
     "status": "ok",
     "timestamp": 1679582413782,
     "user": {
      "displayName": "Jorge Mimoso",
      "userId": "07607860909645255176"
     },
     "user_tz": 0
    },
    "id": "western-citation",
    "outputId": "3b552787-8e33-4a5a-b927-5eee61e31ca4"
   },
   "outputs": [],
   "source": [
    "#!pip install rl-agents==0.1.1\n",
    "#!pip install tensorflow\n",
    "#!pip install gym\n",
    "#!pip install keras\n",
    "#!pip install --user keras-rl2\n",
    "#!pip install pygame\n",
    "#conda install swig -> this should be installed in the Anaconda command line\n",
    "#conda install -c conda-forge box2d-py ->  this should be installed in the Anaconda command line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accomplished-armenia",
   "metadata": {
    "id": "accomplished-armenia"
   },
   "source": [
    "# 1. Test Random Environment with OpenAI Gym\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hindu-ceremony",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4502,
     "status": "ok",
     "timestamp": 1679582555308,
     "user": {
      "displayName": "Jorge Mimoso",
      "userId": "07607860909645255176"
     },
     "user_tz": 0
    },
    "id": "hindu-ceremony",
    "outputId": "5c29a6db-05ed-4d6f-f742-1f3288c6e030"
   },
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "#print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "import gym\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "russian-nudist",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 651,
     "status": "ok",
     "timestamp": 1679592616304,
     "user": {
      "displayName": "Jorge Mimoso",
      "userId": "07607860909645255176"
     },
     "user_tz": 0
    },
    "id": "russian-nudist",
    "outputId": "6e5c79e7-aa7a-4deb-fa78-c3903d2dc08e"
   },
   "outputs": [],
   "source": [
    "#Game being used\n",
    "env = gym.make('LunarLander-v2')# ->https://gymnasium.farama.org/environments/box2d/lunar_lander/\n",
    "\n",
    "\n",
    "states = env.observation_space.shape[0]\n",
    "\n",
    "#Number of actions available.\n",
    "actions = env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "detected-registration",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 259,
     "status": "ok",
     "timestamp": 1679582569422,
     "user": {
      "displayName": "Jorge Mimoso",
      "userId": "07607860909645255176"
     },
     "user_tz": 0
    },
    "id": "detected-registration",
    "outputId": "bf55b49e-4968-4e16-ca03-a1192c99db27",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(states)\n",
    "print(actions.n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "atmospheric-intersection",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 953
    },
    "executionInfo": {
     "elapsed": 15705,
     "status": "ok",
     "timestamp": 1679592641808,
     "user": {
      "displayName": "Jorge Mimoso",
      "userId": "07607860909645255176"
     },
     "user_tz": 0
    },
    "id": "atmospheric-intersection",
    "outputId": "ece43b76-34d6-4523-f391-c062856b42e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:-114.8986943646137\n",
      "Episode:2 Score:-75.1088244748039\n",
      "Episode:3 Score:-127.97116196571781\n",
      "Episode:4 Score:-64.01866857137395\n",
      "Episode:5 Score:-319.84008999313244\n",
      "Episode:6 Score:-82.25442151952902\n",
      "Episode:7 Score:-93.66057245408658\n",
      "Episode:8 Score:-186.33578532754603\n",
      "Episode:9 Score:-85.2483761817838\n",
      "Episode:10 Score:-286.88224362378065\n"
     ]
    }
   ],
   "source": [
    "#Env setting with random behaviour\n",
    "episodes = 10\n",
    "for episode in range(1,episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "        \n",
    "    while not done:\n",
    "        env.render()\n",
    "        n_state, reward, done, info = env.step(env.action_space.sample())\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'.format(episode,score))\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-edgar",
   "metadata": {
    "id": "blocked-edgar"
   },
   "source": [
    "# 2. Create a Deep Learning Model with Keras\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "infectious-smile",
   "metadata": {
    "executionInfo": {
     "elapsed": 2595,
     "status": "ok",
     "timestamp": 1679582618702,
     "user": {
      "displayName": "Jorge Mimoso",
      "userId": "07607860909645255176"
     },
     "user_tz": 0
    },
    "id": "infectious-smile"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from keras import __version__\n",
    "tf.keras.__version__ = __version__\n",
    "\n",
    "from rl.agents import SARSAAgent #check diferent agents -> https://keras-rl.readthedocs.io/en/latest/\n",
    "from rl.agents import DQNAgent #check diferent agents -> https://keras-rl.readthedocs.io/en/latest/\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4pGN9CZ8D9-c",
   "metadata": {
    "executionInfo": {
     "elapsed": 244,
     "status": "ok",
     "timestamp": 1679592647800,
     "user": {
      "displayName": "Jorge Mimoso",
      "userId": "07607860909645255176"
     },
     "user_tz": 0
    },
    "id": "4pGN9CZ8D9-c"
   },
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    model = Sequential() \n",
    "    model.add(Flatten(input_shape=(1,) + env.observation_space.shape)) \n",
    "    model.add(Dense(200,activation='relu'))\n",
    "    model.add(Dense(200,activation='relu'))\n",
    "    model.add(Dense(actions,activation='linear'))    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "jboF2gqFXE20",
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1679587076588,
     "user": {
      "displayName": "Jorge Mimoso",
      "userId": "07607860909645255176"
     },
     "user_tz": 0
    },
    "id": "jboF2gqFXE20"
   },
   "outputs": [],
   "source": [
    "#del model #-> Uncoment if Sequential error appears after building the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "E6yWRKzUEHl2",
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1679592649813,
     "user": {
      "displayName": "Jorge Mimoso",
      "userId": "07607860909645255176"
     },
     "user_tz": 0
    },
    "id": "E6yWRKzUEHl2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Jorge Mimoso\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "model = build_model(states,actions.n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1PGADNmqEJYj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 293,
     "status": "ok",
     "timestamp": 1679588888046,
     "user": {
      "displayName": "Jorge Mimoso",
      "userId": "07607860909645255176"
     },
     "user_tz": 0
    },
    "id": "1PGADNmqEJYj",
    "outputId": "dd06bce0-e780-4dd1-c705-393f759b533f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               1152      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 11,620\n",
      "Trainable params: 11,620\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sxjw_l-RIQDu",
   "metadata": {
    "id": "sxjw_l-RIQDu"
   },
   "source": [
    "# 3. Build Agent with Keras Neural Network\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "Ny-QPwcrEOHk",
   "metadata": {
    "executionInfo": {
     "elapsed": 203,
     "status": "ok",
     "timestamp": 1679592658294,
     "user": {
      "displayName": "Jorge Mimoso",
      "userId": "07607860909645255176"
     },
     "user_tz": 0
    },
    "id": "Ny-QPwcrEOHk"
   },
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = EpsGreedyQPolicy()\n",
    "    #earlystop = EarlyStopping(monitor = 'episode_reward', min_delta=.1, patience=5, verbose=1, mode='auto') \n",
    "    memory = SequentialMemory(limit=50000,window_length=1)\n",
    "    #callbacks = [earlystop] \n",
    "    nb_steps_warmup = 1000 \n",
    "    target_model_update = .02 \n",
    "    #gamma = .99 \n",
    "    #epochs = training_steps/1000 \n",
    "    #decay = float(lr/epochs) \n",
    "    dqn = DQNAgent(model=model, nb_actions=actions, memory=memory, nb_steps_warmup=nb_steps_warmup, target_model_update = target_model_update, policy=policy)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "JWH3hy49CWm9",
   "metadata": {
    "executionInfo": {
     "elapsed": 2038,
     "status": "ok",
     "timestamp": 1679592664324,
     "user": {
      "displayName": "Jorge Mimoso",
      "userId": "07607860909645255176"
     },
     "user_tz": 0
    },
    "id": "JWH3hy49CWm9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 2000000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 271s 27ms/step - reward: -0.9349\n",
      "43 episodes - episode_reward: -219.430 [-539.268, -2.123] - loss: 13.084 - mae: 32.767 - mean_q: -5.392\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 317s 32ms/step - reward: -0.0534\n",
      "11 episodes - episode_reward: -37.325 [-175.087, 186.687] - loss: 3.925 - mae: 31.711 - mean_q: 30.179\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 310s 31ms/step - reward: -0.0055\n",
      "11 episodes - episode_reward: -11.473 [-234.867, 175.665] - loss: 3.262 - mae: 29.404 - mean_q: 32.764\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 314s 31ms/step - reward: 0.03440s - reward: 0.034\n",
      "10 episodes - episode_reward: 35.986 [-76.513, 240.205] - loss: 3.525 - mae: 28.646 - mean_q: 33.802\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 305s 30ms/step - reward: 0.11790s - reward: \n",
      "15 episodes - episode_reward: 73.094 [-196.732, 220.540] - loss: 4.334 - mae: 33.573 - mean_q: 41.455\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 305s 31ms/step - reward: 0.09140s - rew\n",
      "16 episodes - episode_reward: 61.998 [-552.890, 248.868] - loss: 4.879 - mae: 32.880 - mean_q: 41.491\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 302s 30ms/step - reward: 0.0073\n",
      "13 episodes - episode_reward: 5.664 [-316.290, 228.693] - loss: 5.405 - mae: 31.701 - mean_q: 40.159\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 307s 31ms/step - reward: 0.2266\n",
      "15 episodes - episode_reward: 149.808 [-43.717, 269.194] - loss: 5.090 - mae: 35.062 - mean_q: 44.912\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 303s 30ms/step - reward: 0.3141\n",
      "17 episodes - episode_reward: 184.054 [-73.377, 266.976] - loss: 4.729 - mae: 36.622 - mean_q: 47.301\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 257s 26ms/step - reward: 0.14440s - reward: 0\n",
      "15 episodes - episode_reward: 97.073 [-301.375, 273.536] - loss: 4.664 - mae: 36.077 - mean_q: 46.760\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 113s 11ms/step - reward: 0.16561s\n",
      "14 episodes - episode_reward: 116.751 [-274.482, 282.605] - loss: 4.225 - mae: 33.653 - mean_q: 43.580\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 114s 11ms/step - reward: 0.2074\n",
      "16 episodes - episode_reward: 129.615 [-145.773, 239.101] - loss: 4.822 - mae: 35.787 - mean_q: 46.709\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 115s 11ms/step - reward: 0.3552\n",
      "18 episodes - episode_reward: 197.773 [-163.261, 254.111] - loss: 4.449 - mae: 34.527 - mean_q: 45.420\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 126s 13ms/step - reward: 0.24451s - reward: 0.24\n",
      "15 episodes - episode_reward: 165.515 [-33.951, 277.866] - loss: 5.139 - mae: 33.546 - mean_q: 44.082\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 119s 12ms/step - reward: 0.3039\n",
      "16 episodes - episode_reward: 185.394 [-37.517, 261.797] - loss: 4.435 - mae: 34.976 - mean_q: 46.155\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 111s 11ms/step - reward: 0.4014\n",
      "20 episodes - episode_reward: 202.077 [-79.215, 251.614] - loss: 4.432 - mae: 36.157 - mean_q: 47.751\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 115s 12ms/step - reward: 0.3457\n",
      "18 episodes - episode_reward: 188.668 [-199.061, 252.829] - loss: 3.716 - mae: 36.235 - mean_q: 47.954\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 122s 12ms/step - reward: 0.29590s - reward:\n",
      "19 episodes - episode_reward: 157.490 [-324.462, 290.515] - loss: 4.254 - mae: 39.369 - mean_q: 52.203\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 124s 12ms/step - reward: 0.426611s  - ETA: 5s - rew\n",
      "19 episodes - episode_reward: 225.493 [123.375, 282.031] - loss: 4.125 - mae: 39.094 - mean_q: 51.864\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 124s 12ms/step - reward: 0.45568s - reward: 0. - ETA: 8s - rewar - ETA: 6s -  - ETA: 0s - rewar - ETA: 0s - reward: \n",
      "20 episodes - episode_reward: 228.469 [86.146, 284.146] - loss: 4.064 - mae: 37.835 - mean_q: 50.035\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 127s 13ms/step - reward: 0.2558\n",
      "18 episodes - episode_reward: 140.791 [-250.577, 276.791] - loss: 4.702 - mae: 43.758 - mean_q: 58.266\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      "10000/10000 [==============================] - 89s 9ms/step - reward: 0.4181\n",
      "19 episodes - episode_reward: 216.689 [36.979, 274.040] - loss: 4.358 - mae: 40.851 - mean_q: 54.254\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      "10000/10000 [==============================] - 89s 9ms/step - reward: 0.4676\n",
      "20 episodes - episode_reward: 232.787 [49.647, 294.052] - loss: 4.267 - mae: 42.992 - mean_q: 57.283\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      "10000/10000 [==============================] - 90s 9ms/step - reward: 0.3627\n",
      "20 episodes - episode_reward: 185.249 [-39.846, 282.101] - loss: 4.854 - mae: 42.536 - mean_q: 56.710\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      "10000/10000 [==============================] - 90s 9ms/step - reward: 0.4220\n",
      "22 episodes - episode_reward: 195.691 [-8.290, 303.200] - loss: 4.681 - mae: 41.206 - mean_q: 54.944\n",
      "\n",
      "Interval 26 (250000 steps performed)\n",
      "10000/10000 [==============================] - 91s 9ms/step - reward: 0.4489\n",
      "20 episodes - episode_reward: 220.289 [36.742, 292.395] - loss: 4.460 - mae: 42.107 - mean_q: 56.120\n",
      "\n",
      "Interval 27 (260000 steps performed)\n",
      "10000/10000 [==============================] - 93s 9ms/step - reward: 0.4672\n",
      "23 episodes - episode_reward: 204.019 [-213.344, 298.088] - loss: 4.370 - mae: 42.507 - mean_q: 56.632\n",
      "\n",
      "Interval 28 (270000 steps performed)\n",
      "10000/10000 [==============================] - 94s 9ms/step - reward: 0.4687\n",
      "21 episodes - episode_reward: 218.881 [3.878, 284.055] - loss: 4.669 - mae: 42.476 - mean_q: 56.618\n",
      "\n",
      "Interval 29 (280000 steps performed)\n",
      "10000/10000 [==============================] - 108s 11ms/step - reward: 0.3163\n",
      "21 episodes - episode_reward: 149.626 [-537.070, 298.609] - loss: 5.557 - mae: 43.255 - mean_q: 57.503\n",
      "\n",
      "Interval 30 (290000 steps performed)\n",
      "10000/10000 [==============================] - 138s 14ms/step - reward: 0.49284s - reward: 0.5 - ETA - ETA: 0s - reward: \n",
      "23 episodes - episode_reward: 217.001 [-198.783, 289.285] - loss: 5.018 - mae: 42.555 - mean_q: 56.708\n",
      "\n",
      "Interval 31 (300000 steps performed)\n",
      "10000/10000 [==============================] - 142s 14ms/step - reward: 0.51982s - \n",
      "23 episodes - episode_reward: 225.610 [-211.183, 293.841] - loss: 5.012 - mae: 45.828 - mean_q: 61.037\n",
      "\n",
      "Interval 32 (310000 steps performed)\n",
      "10000/10000 [==============================] - 141s 14ms/step - reward: 0.42033s - rew - ETA: 0s - \n",
      "27 episodes - episode_reward: 156.119 [-354.240, 314.960] - loss: 5.741 - mae: 48.443 - mean_q: 64.547\n",
      "\n",
      "Interval 33 (320000 steps performed)\n",
      "10000/10000 [==============================] - 146s 15ms/step - reward: 0.33487s - reward: 0 - ET - ETA - ETA: 1s - reward - ETA: 1s - re - ETA: 0s - reward: \n",
      "21 episodes - episode_reward: 157.027 [-324.103, 295.013] - loss: 5.410 - mae: 48.321 - mean_q: 64.479\n",
      "\n",
      "Interval 34 (330000 steps performed)\n",
      "10000/10000 [==============================] - 149s 15ms/step - reward: 0.49871s - reward: 0.503 - ETA: 1s -  - ETA: 0s - rewa\n",
      "22 episodes - episode_reward: 226.273 [-43.211, 299.110] - loss: 5.500 - mae: 49.166 - mean_q: 65.650\n",
      "\n",
      "Interval 35 (340000 steps performed)\n",
      "10000/10000 [==============================] - 150s 15ms/step - reward: 0.5856\n",
      "25 episodes - episode_reward: 240.750 [52.983, 295.767] - loss: 5.832 - mae: 46.617 - mean_q: 62.155\n",
      "\n",
      "Interval 36 (350000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 153s 15ms/step - reward: 0.5559\n",
      "22 episodes - episode_reward: 246.709 [115.663, 294.787] - loss: 5.067 - mae: 45.695 - mean_q: 61.002\n",
      "\n",
      "Interval 37 (360000 steps performed)\n",
      "10000/10000 [==============================] - 156s 16ms/step - reward: 0.5680\n",
      "25 episodes - episode_reward: 230.164 [-230.948, 301.523] - loss: 5.308 - mae: 46.812 - mean_q: 62.433\n",
      "\n",
      "Interval 38 (370000 steps performed)\n",
      "10000/10000 [==============================] - 157s 16ms/step - reward: 0.48642s - reward: 0.482 - ETA: \n",
      "24 episodes - episode_reward: 200.434 [-130.461, 294.533] - loss: 5.025 - mae: 46.105 - mean_q: 61.479\n",
      "\n",
      "Interval 39 (380000 steps performed)\n",
      "10000/10000 [==============================] - 161s 16ms/step - reward: 0.50143s - reward: - ETA: 2s - rewa - ETA: 1s -  - ETA: 1s - reward:  - ETA: 0s -\n",
      "22 episodes - episode_reward: 229.053 [-5.638, 299.016] - loss: 7.422 - mae: 48.030 - mean_q: 63.757\n",
      "\n",
      "Interval 40 (390000 steps performed)\n",
      "10000/10000 [==============================] - 163s 16ms/step - reward: 0.60070s - reward\n",
      "26 episodes - episode_reward: 229.279 [-91.206, 309.421] - loss: 5.620 - mae: 46.280 - mean_q: 61.798\n",
      "\n",
      "Interval 41 (400000 steps performed)\n",
      "10000/10000 [==============================] - 164s 16ms/step - reward: 0.5369\n",
      "24 episodes - episode_reward: 227.049 [-52.688, 310.391] - loss: 5.252 - mae: 47.883 - mean_q: 63.698\n",
      "\n",
      "Interval 42 (410000 steps performed)\n",
      "10000/10000 [==============================] - 188s 19ms/step - reward: 0.6041\n",
      "27 episodes - episode_reward: 223.980 [-160.937, 293.931] - loss: 5.219 - mae: 47.109 - mean_q: 62.930\n",
      "\n",
      "Interval 43 (420000 steps performed)\n",
      "10000/10000 [==============================] - 456s 46ms/step - reward: 0.6465\n",
      "26 episodes - episode_reward: 247.150 [15.822, 299.290] - loss: 5.149 - mae: 46.100 - mean_q: 61.480\n",
      "\n",
      "Interval 44 (430000 steps performed)\n",
      "10000/10000 [==============================] - 461s 46ms/step - reward: 0.4416\n",
      "25 episodes - episode_reward: 178.233 [-319.509, 294.172] - loss: 5.915 - mae: 47.423 - mean_q: 63.128\n",
      "\n",
      "Interval 45 (440000 steps performed)\n",
      "10000/10000 [==============================] - 468s 47ms/step - reward: 0.6181\n",
      "26 episodes - episode_reward: 232.114 [-11.080, 294.538] - loss: 5.600 - mae: 46.390 - mean_q: 61.903\n",
      "\n",
      "Interval 46 (450000 steps performed)\n",
      "10000/10000 [==============================] - 468s 47ms/step - reward: 0.5954\n",
      "26 episodes - episode_reward: 230.901 [-40.315, 302.808] - loss: 5.146 - mae: 47.481 - mean_q: 63.378\n",
      "\n",
      "Interval 47 (460000 steps performed)\n",
      "10000/10000 [==============================] - 461s 46ms/step - reward: 0.5480\n",
      "24 episodes - episode_reward: 233.968 [21.283, 282.793] - loss: 5.315 - mae: 46.577 - mean_q: 62.163\n",
      "\n",
      "Interval 48 (470000 steps performed)\n",
      "10000/10000 [==============================] - 471s 47ms/step - reward: 0.5797\n",
      "24 episodes - episode_reward: 241.044 [-27.753, 294.760] - loss: 4.912 - mae: 46.417 - mean_q: 61.957\n",
      "\n",
      "Interval 49 (480000 steps performed)\n",
      "10000/10000 [==============================] - 475s 47ms/step - reward: 0.6447\n",
      "25 episodes - episode_reward: 255.352 [209.016, 284.769] - loss: 5.671 - mae: 47.100 - mean_q: 62.844\n",
      "\n",
      "Interval 50 (490000 steps performed)\n",
      "10000/10000 [==============================] - 483s 48ms/step - reward: 0.5237\n",
      "21 episodes - episode_reward: 251.599 [174.248, 287.102] - loss: 5.504 - mae: 48.583 - mean_q: 64.978\n",
      "\n",
      "Interval 51 (500000 steps performed)\n",
      "10000/10000 [==============================] - 486s 49ms/step - reward: 0.7068\n",
      "27 episodes - episode_reward: 256.948 [207.547, 294.358] - loss: 5.610 - mae: 47.902 - mean_q: 63.954\n",
      "\n",
      "Interval 52 (510000 steps performed)\n",
      "10000/10000 [==============================] - 491s 49ms/step - reward: 0.6654\n",
      "28 episodes - episode_reward: 239.378 [-34.583, 288.492] - loss: 5.347 - mae: 47.639 - mean_q: 63.648\n",
      "\n",
      "Interval 53 (520000 steps performed)\n",
      "10000/10000 [==============================] - 495s 50ms/step - reward: 0.6597\n",
      "27 episodes - episode_reward: 248.704 [-28.685, 303.673] - loss: 5.285 - mae: 47.941 - mean_q: 64.204\n",
      "\n",
      "Interval 54 (530000 steps performed)\n",
      "10000/10000 [==============================] - 501s 50ms/step - reward: 0.6852\n",
      "27 episodes - episode_reward: 254.679 [216.282, 294.646] - loss: 5.942 - mae: 47.745 - mean_q: 63.720\n",
      "\n",
      "Interval 55 (540000 steps performed)\n",
      "10000/10000 [==============================] - 505s 50ms/step - reward: 0.6163\n",
      "26 episodes - episode_reward: 234.159 [12.761, 294.912] - loss: 5.590 - mae: 48.813 - mean_q: 65.251\n",
      "\n",
      "Interval 56 (550000 steps performed)\n",
      "10000/10000 [==============================] - 503s 50ms/step - reward: 0.4825\n",
      "27 episodes - episode_reward: 180.976 [-315.195, 302.941] - loss: 6.321 - mae: 49.045 - mean_q: 65.319\n",
      "\n",
      "Interval 57 (560000 steps performed)\n",
      "10000/10000 [==============================] - 516s 52ms/step - reward: -0.8809\n",
      "41 episodes - episode_reward: -216.098 [-1719.449, 262.525] - loss: 12.469 - mae: 64.190 - mean_q: 86.072\n",
      "\n",
      "Interval 58 (570000 steps performed)\n",
      "10000/10000 [==============================] - 507s 51ms/step - reward: 0.4086\n",
      "35 episodes - episode_reward: 117.709 [-148.229, 290.897] - loss: 6.708 - mae: 51.620 - mean_q: 68.941\n",
      "\n",
      "Interval 59 (580000 steps performed)\n",
      "10000/10000 [==============================] - 513s 51ms/step - reward: 0.4340\n",
      "29 episodes - episode_reward: 144.624 [-208.038, 294.388] - loss: 7.346 - mae: 50.709 - mean_q: 67.595\n",
      "\n",
      "Interval 60 (590000 steps performed)\n",
      "10000/10000 [==============================] - 393s 39ms/step - reward: -0.0426\n",
      "34 episodes - episode_reward: -11.227 [-796.002, 289.338] - loss: 10.979 - mae: 52.809 - mean_q: 70.621\n",
      "\n",
      "Interval 61 (600000 steps performed)\n",
      "10000/10000 [==============================] - 330s 33ms/step - reward: 0.2053\n",
      "34 episodes - episode_reward: 66.513 [-478.959, 290.968] - loss: 11.607 - mae: 55.034 - mean_q: 73.377\n",
      "\n",
      "Interval 62 (610000 steps performed)\n",
      "10000/10000 [==============================] - 335s 33ms/step - reward: -0.1067\n",
      "29 episodes - episode_reward: -40.441 [-985.561, 291.797] - loss: 11.380 - mae: 53.995 - mean_q: 72.029\n",
      "\n",
      "Interval 63 (620000 steps performed)\n",
      "10000/10000 [==============================] - 24279s 2s/step - reward: 0.5831- ETA: 5:34 - rewa - ETA: 4:17 - reward: 0 - ETA: 3:38 - re - E\n",
      "25 episodes - episode_reward: 226.776 [8.497, 310.713] - loss: 10.190 - mae: 52.757 - mean_q: 70.141\n",
      "\n",
      "Interval 64 (630000 steps performed)\n",
      "10000/10000 [==============================] - ETA: 0s - reward: 0.516 - 221s 22ms/step - reward: 0.5161\n",
      "26 episodes - episode_reward: 202.520 [-223.990, 289.262] - loss: 9.510 - mae: 50.437 - mean_q: 66.978\n",
      "\n",
      "Interval 65 (640000 steps performed)\n",
      "10000/10000 [==============================] - 223s 22ms/step - reward: 0.40391s - reward: 0.4\n",
      "25 episodes - episode_reward: 158.802 [-144.890, 287.492] - loss: 7.226 - mae: 48.398 - mean_q: 64.250\n",
      "\n",
      "Interval 66 (650000 steps performed)\n",
      "10000/10000 [==============================] - 215s 22ms/step - reward: 0.18663s - reward: 0.1 - ETA: 3s - reward: 0 - ETA:\n",
      "21 episodes - episode_reward: 93.292 [-1760.862, 287.372] - loss: 9.452 - mae: 52.278 - mean_q: 69.433\n",
      "\n",
      "Interval 67 (660000 steps performed)\n",
      "10000/10000 [==============================] - 215s 22ms/step - reward: 0.6069\n",
      "28 episodes - episode_reward: 211.464 [-48.401, 305.645] - loss: 8.667 - mae: 51.762 - mean_q: 68.907\n",
      "\n",
      "Interval 68 (670000 steps performed)\n",
      "10000/10000 [==============================] - 217s 22ms/step - reward: -0.3289A: 2:59 - rewar - ETA: 2:57 - reward: 0.707 - ETA: 2:57 - reward: 0. - ETA: 2:56 - reward: - ETA: 2:56 - reward: - ETA: 1s - rewar - ETA: 1s - - ETA: \n",
      "38 episodes - episode_reward: -82.484 [-851.071, 302.832] - loss: 68.906 - mae: 61.168 - mean_q: 81.689\n",
      "\n",
      "Interval 69 (680000 steps performed)\n",
      "10000/10000 [==============================] - 220s 22ms/step - reward: -0.1886\n",
      "24 episodes - episode_reward: -83.597 [-1182.079, 298.578] - loss: 13.598 - mae: 55.597 - mean_q: 74.114\n",
      "\n",
      "Interval 70 (690000 steps performed)\n",
      "10000/10000 [==============================] - 224s 22ms/step - reward: 0.6314: 0s - r\n",
      "31 episodes - episode_reward: 209.286 [-55.667, 303.798] - loss: 8.255 - mae: 50.674 - mean_q: 67.275\n",
      "\n",
      "Interval 71 (700000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 5667s 567ms/step - reward: 0.5544\n",
      "35 episodes - episode_reward: 158.420 [-43.493, 293.137] - loss: 7.413 - mae: 52.239 - mean_q: 69.539\n",
      "\n",
      "Interval 72 (710000 steps performed)\n",
      "10000/10000 [==============================] - 235s 23ms/step - reward: 0.3605\n",
      "26 episodes - episode_reward: 137.791 [-531.280, 301.426] - loss: 7.255 - mae: 53.500 - mean_q: 71.199\n",
      "\n",
      "Interval 73 (720000 steps performed)\n",
      "10000/10000 [==============================] - 171s 17ms/step - reward: 0.3623\n",
      "29 episodes - episode_reward: 126.777 [-195.320, 307.376] - loss: 8.199 - mae: 51.410 - mean_q: 68.585\n",
      "\n",
      "Interval 74 (730000 steps performed)\n",
      "10000/10000 [==============================] - 176s 18ms/step - reward: -0.8796\n",
      "42 episodes - episode_reward: -213.498 [-1237.516, 290.463] - loss: 14.063 - mae: 54.695 - mean_q: 73.056\n",
      "\n",
      "Interval 75 (740000 steps performed)\n",
      "10000/10000 [==============================] - 174s 17ms/step - reward: 0.2568\n",
      "33 episodes - episode_reward: 77.609 [-88.810, 275.961] - loss: 9.207 - mae: 51.874 - mean_q: 68.618\n",
      "\n",
      "Interval 76 (750000 steps performed)\n",
      "10000/10000 [==============================] - 172s 17ms/step - reward: 0.2459\n",
      "19 episodes - episode_reward: 129.991 [-83.098, 293.060] - loss: 9.364 - mae: 51.909 - mean_q: 69.030\n",
      "\n",
      "Interval 77 (760000 steps performed)\n",
      "10000/10000 [==============================] - 174s 17ms/step - reward: 0.2551\n",
      "19 episodes - episode_reward: 132.355 [-69.599, 276.566] - loss: 9.510 - mae: 51.906 - mean_q: 68.969\n",
      "\n",
      "Interval 78 (770000 steps performed)\n",
      "10000/10000 [==============================] - 179s 18ms/step - reward: 0.4325\n",
      "23 episodes - episode_reward: 193.711 [-96.855, 295.422] - loss: 9.217 - mae: 50.692 - mean_q: 67.553\n",
      "\n",
      "Interval 79 (780000 steps performed)\n",
      "10000/10000 [==============================] - 174s 17ms/step - reward: 0.3690\n",
      "24 episodes - episode_reward: 148.569 [-94.281, 295.185] - loss: 8.118 - mae: 50.798 - mean_q: 67.706\n",
      "\n",
      "Interval 80 (790000 steps performed)\n",
      "10000/10000 [==============================] - 226s 23ms/step - reward: 0.2949\n",
      "28 episodes - episode_reward: 109.042 [-145.609, 273.553] - loss: 9.749 - mae: 53.555 - mean_q: 71.448\n",
      "\n",
      "Interval 81 (800000 steps performed)\n",
      "10000/10000 [==============================] - 284s 28ms/step - reward: 0.42960s - re\n",
      "25 episodes - episode_reward: 168.929 [-192.098, 300.841] - loss: 10.420 - mae: 53.508 - mean_q: 71.376\n",
      "\n",
      "Interval 82 (810000 steps performed)\n",
      "10000/10000 [==============================] - 286s 29ms/step - reward: 0.2507\n",
      "21 episodes - episode_reward: 117.151 [-83.271, 291.050] - loss: 29.172 - mae: 52.991 - mean_q: 70.782\n",
      "\n",
      "Interval 83 (820000 steps performed)\n",
      "10000/10000 [==============================] - 312s 31ms/step - reward: 0.32829s - rew - ETA: 8s - reward: 0.3 - \n",
      "18 episodes - episode_reward: 182.030 [-106.372, 273.418] - loss: 12.001 - mae: 54.464 - mean_q: 72.738\n",
      "\n",
      "Interval 84 (830000 steps performed)\n",
      "10000/10000 [==============================] - 329s 33ms/step - reward: 0.3963- ETA: 0s - r\n",
      "20 episodes - episode_reward: 199.575 [-16.960, 293.421] - loss: 9.435 - mae: 54.941 - mean_q: 73.279\n",
      "\n",
      "Interval 85 (840000 steps performed)\n",
      "10000/10000 [==============================] - 271s 27ms/step - reward: 0.37501s\n",
      "20 episodes - episode_reward: 188.816 [-304.771, 298.290] - loss: 8.711 - mae: 51.144 - mean_q: 68.143\n",
      "\n",
      "Interval 86 (850000 steps performed)\n",
      "10000/10000 [==============================] - 228s 23ms/step - reward: 0.4728\n",
      "24 episodes - episode_reward: 196.023 [-75.682, 275.395] - loss: 12.534 - mae: 52.212 - mean_q: 69.498\n",
      "\n",
      "Interval 87 (860000 steps performed)\n",
      " 7140/10000 [====================>.........] - ETA: 1:05 - reward: 0.4709- Edone, took 51427.668 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e100807080>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #Adam._name = 'hey' ## use in case of error mentioning this parameter as null-\n",
    "dqn = build_agent(model,actions.n)\n",
    "\n",
    "lr = .0001 \n",
    "dqn.compile(Adam(learning_rate=lr), metrics=['mae'],)\n",
    "\n",
    "training_steps = 2000000\n",
    "\n",
    "dqn.fit(env, nb_steps=training_steps, visualize=False, verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "Fd_JxTR2EOwi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52082,
     "status": "ok",
     "timestamp": 1679592722687,
     "user": {
      "displayName": "Jorge Mimoso",
      "userId": "07607860909645255176"
     },
     "user_tz": 0
    },
    "id": "Fd_JxTR2EOwi",
    "outputId": "b07d1b70-db9e-4657-c680-6a0524e50503"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 100 episodes ...\n",
      "Episode 1: reward: 278.615, steps: 240\n",
      "Episode 2: reward: 228.422, steps: 212\n",
      "Episode 3: reward: 287.743, steps: 311\n",
      "Episode 4: reward: 245.099, steps: 224\n",
      "Episode 5: reward: 285.469, steps: 204\n",
      "Episode 6: reward: 278.986, steps: 286\n",
      "Episode 7: reward: 251.033, steps: 370\n",
      "Episode 8: reward: 267.478, steps: 272\n",
      "Episode 9: reward: 88.778, steps: 1000\n",
      "Episode 10: reward: 274.999, steps: 270\n",
      "Episode 11: reward: 265.813, steps: 244\n",
      "Episode 12: reward: 245.879, steps: 263\n",
      "Episode 13: reward: 264.490, steps: 236\n",
      "Episode 14: reward: 274.809, steps: 296\n",
      "Episode 15: reward: 291.175, steps: 278\n",
      "Episode 16: reward: 277.914, steps: 229\n",
      "Episode 17: reward: 242.387, steps: 211\n",
      "Episode 18: reward: 269.509, steps: 226\n",
      "Episode 19: reward: 271.139, steps: 336\n",
      "Episode 20: reward: 300.252, steps: 318\n",
      "Episode 21: reward: 245.632, steps: 221\n",
      "Episode 22: reward: 239.106, steps: 614\n",
      "Episode 23: reward: 287.416, steps: 259\n",
      "Episode 24: reward: 271.385, steps: 259\n",
      "Episode 25: reward: 250.941, steps: 271\n",
      "Episode 26: reward: 285.880, steps: 312\n",
      "Episode 27: reward: 232.524, steps: 302\n",
      "Episode 28: reward: 286.884, steps: 312\n",
      "Episode 29: reward: 248.007, steps: 252\n",
      "Episode 30: reward: 236.022, steps: 253\n",
      "Episode 31: reward: 264.484, steps: 356\n",
      "Episode 32: reward: 289.295, steps: 279\n",
      "Episode 33: reward: 280.701, steps: 202\n",
      "Episode 34: reward: 265.599, steps: 246\n",
      "Episode 35: reward: 278.934, steps: 311\n",
      "Episode 36: reward: 254.545, steps: 285\n",
      "Episode 37: reward: 277.098, steps: 232\n",
      "Episode 38: reward: 228.777, steps: 490\n",
      "Episode 39: reward: 275.748, steps: 942\n",
      "Episode 40: reward: 272.601, steps: 281\n",
      "Episode 41: reward: 250.490, steps: 288\n",
      "Episode 42: reward: 251.845, steps: 348\n",
      "Episode 43: reward: 253.750, steps: 382\n",
      "Episode 44: reward: 212.598, steps: 384\n",
      "Episode 45: reward: 267.665, steps: 336\n",
      "Episode 46: reward: 253.435, steps: 247\n",
      "Episode 47: reward: 233.639, steps: 217\n",
      "Episode 48: reward: 264.083, steps: 201\n",
      "Episode 49: reward: 284.237, steps: 512\n",
      "Episode 50: reward: 281.068, steps: 391\n",
      "Episode 51: reward: 238.949, steps: 287\n",
      "Episode 52: reward: 289.145, steps: 212\n",
      "Episode 53: reward: 235.521, steps: 319\n",
      "Episode 54: reward: 289.249, steps: 339\n",
      "Episode 55: reward: 293.568, steps: 286\n",
      "Episode 56: reward: 270.390, steps: 325\n",
      "Episode 57: reward: 275.035, steps: 282\n",
      "Episode 58: reward: 288.738, steps: 263\n",
      "Episode 59: reward: 273.872, steps: 256\n",
      "Episode 60: reward: 271.755, steps: 332\n",
      "Episode 61: reward: 258.523, steps: 272\n",
      "Episode 62: reward: 274.328, steps: 294\n",
      "Episode 63: reward: 262.350, steps: 241\n",
      "Episode 64: reward: 285.080, steps: 289\n",
      "Episode 65: reward: 280.014, steps: 326\n",
      "Episode 66: reward: 260.557, steps: 264\n",
      "Episode 67: reward: 246.095, steps: 418\n",
      "Episode 68: reward: 242.563, steps: 228\n",
      "Episode 69: reward: 264.498, steps: 261\n",
      "Episode 70: reward: 256.840, steps: 271\n",
      "Episode 71: reward: 282.915, steps: 392\n",
      "Episode 72: reward: 253.846, steps: 278\n",
      "Episode 73: reward: 267.439, steps: 267\n",
      "Episode 74: reward: 209.510, steps: 295\n",
      "Episode 75: reward: 113.077, steps: 1000\n",
      "Episode 76: reward: 244.222, steps: 502\n",
      "Episode 77: reward: 232.881, steps: 253\n",
      "Episode 78: reward: 239.929, steps: 655\n",
      "Episode 79: reward: 272.304, steps: 236\n",
      "Episode 80: reward: 259.255, steps: 225\n",
      "Episode 81: reward: 243.399, steps: 261\n",
      "Episode 82: reward: 243.477, steps: 208\n",
      "Episode 83: reward: -2.401, steps: 200\n",
      "Episode 84: reward: 248.046, steps: 260\n",
      "Episode 85: reward: 290.542, steps: 309\n",
      "Episode 86: reward: 276.692, steps: 408\n",
      "Episode 87: reward: 262.909, steps: 257\n",
      "Episode 88: reward: 271.440, steps: 285\n",
      "Episode 89: reward: 245.592, steps: 300\n",
      "Episode 90: reward: 263.910, steps: 285\n",
      "Episode 91: reward: 271.086, steps: 308\n",
      "Episode 92: reward: 286.748, steps: 317\n",
      "Episode 93: reward: 264.021, steps: 237\n",
      "Episode 94: reward: 270.216, steps: 306\n",
      "Episode 95: reward: 266.160, steps: 363\n",
      "Episode 96: reward: 282.338, steps: 285\n",
      "Episode 97: reward: 262.210, steps: 270\n",
      "Episode 98: reward: 241.368, steps: 201\n",
      "Episode 99: reward: 264.623, steps: 279\n",
      "Episode 100: reward: 158.589, steps: 1000\n",
      "256.5779067255895\n"
     ]
    }
   ],
   "source": [
    "scores = dqn.test(env, nb_episodes=100, visualize=False)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "G7kb8LE5I8tB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "executionInfo": {
     "elapsed": 27345,
     "status": "error",
     "timestamp": 1679587011164,
     "user": {
      "displayName": "Jorge Mimoso",
      "userId": "07607860909645255176"
     },
     "user_tz": 0
    },
    "id": "G7kb8LE5I8tB",
    "outputId": "f93c47bc-254e-4565-f66c-f3f46c53968e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 15 episodes ...\n",
      "Episode 1: reward: 264.158, steps: 243\n",
      "Episode 2: reward: 159.949, steps: 1000\n",
      "Episode 3: reward: 246.209, steps: 230\n",
      "Episode 4: reward: 246.213, steps: 191\n",
      "Episode 5: reward: 272.052, steps: 376\n",
      "Episode 6: reward: 230.638, steps: 295\n",
      "Episode 7: reward: 275.117, steps: 299\n",
      "Episode 8: reward: 283.678, steps: 246\n",
      "Episode 9: reward: 275.258, steps: 242\n",
      "Episode 10: reward: 291.510, steps: 328\n",
      "Episode 11: reward: 289.308, steps: 268\n",
      "Episode 12: reward: 265.099, steps: 309\n",
      "Episode 13: reward: 274.676, steps: 306\n",
      "Episode 14: reward: 247.326, steps: 323\n",
      "Episode 15: reward: 260.874, steps: 249\n"
     ]
    }
   ],
   "source": [
    "#lets test again with more 15 episodes\n",
    "_ = dqn.test(env, nb_episodes=15, visualize=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "lTah1BG31RTk",
   "metadata": {
    "executionInfo": {
     "elapsed": 665,
     "status": "ok",
     "timestamp": 1679592419292,
     "user": {
      "displayName": "Jorge Mimoso",
      "userId": "07607860909645255176"
     },
     "user_tz": 0
    },
    "id": "lTah1BG31RTk"
   },
   "outputs": [],
   "source": [
    "dqn.save_weights('LunarLander-v2_weights.h5f',overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
